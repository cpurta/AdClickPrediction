{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A, paths\n",
    "data_path = \"./input/\"\n",
    "train = data_path+'clicks_train.csv'               # path to training file\n",
    "test = data_path+'clicks_test.csv'                 # path to testing file\n",
    "submission = 'sub_proba.csv'  # path of to be outputted submission file\n",
    "\n",
    "# B, model\n",
    "alpha = .1  # learning rate\n",
    "beta = 0.   # smoothing parameter for adaptive learning rate\n",
    "L1 = 0.    # L1 regularization, larger value means more regularized\n",
    "L2 = 0.     # L2 regularization, larger value means more regularized\n",
    "\n",
    "# C, feature/hash trick\n",
    "D = 2 ** 20             # number of weights to use\n",
    "interaction = False     # whether to enable poly2 feature interactions\n",
    "\n",
    "# D, training/validation\n",
    "epoch = 1       # learn training data for N passes\n",
    "holdout = 100   # use every N training instance for holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ftrl_proximal(object):\n",
    "    ''' Our main algorithm: Follow the regularized leader - proximal\n",
    "\n",
    "        In short,\n",
    "        this is an adaptive-learning-rate sparse logistic-regression with\n",
    "        efficient L1-L2-regularization\n",
    "\n",
    "        Reference:\n",
    "        http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, alpha, beta, L1, L2, D, interaction):\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "\n",
    "        # feature related parameters\n",
    "        self.D = D\n",
    "        self.interaction = interaction\n",
    "\n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        self.n = [0.] * D\n",
    "        self.z = [0.] * D\n",
    "        self.w = {}\n",
    "\n",
    "    def _indices(self, x):\n",
    "        ''' A helper generator that yields the indices in x\n",
    "\n",
    "            The purpose of this generator is to make the following\n",
    "            code a bit cleaner when doing feature interaction.\n",
    "        '''\n",
    "\n",
    "        # first yield index of the bias term\n",
    "        yield 0\n",
    "\n",
    "        # then yield the normal indices\n",
    "        for index in x:\n",
    "            yield index\n",
    "\n",
    "        # now yield interactions (if applicable)\n",
    "        if self.interaction:\n",
    "            D = self.D\n",
    "            L = len(x)\n",
    "\n",
    "            x = sorted(x)\n",
    "            for i in xrange(L):\n",
    "                for j in xrange(i+1, L):\n",
    "                    # one-hot encode interactions with hash trick\n",
    "                    yield abs(hash(str(x[i]) + '_' + str(x[j]))) % D\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' Get probability estimation on x\n",
    "\n",
    "            INPUT:\n",
    "                x: features\n",
    "\n",
    "            OUTPUT:\n",
    "                probability of p(y = 1 | x; w)\n",
    "        '''\n",
    "\n",
    "        # parameters\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = {}\n",
    "\n",
    "        # wTx is the inner product of w and x\n",
    "        wTx = 0.\n",
    "        for i in self._indices(x):\n",
    "            sign = -1. if z[i] < 0 else 1.  # get sign of z[i]\n",
    "\n",
    "            # build w on the fly using z and n, hence the name - lazy weights\n",
    "            # we are doing this at prediction instead of update time is because\n",
    "            # this allows us for not storing the complete w\n",
    "            if sign * z[i] <= L1:\n",
    "                # w[i] vanishes due to L1 regularization\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                # apply prediction time L1, L2 regularization to z and get w\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "\n",
    "            wTx += w[i]\n",
    "\n",
    "        # cache the current w for update stage\n",
    "        self.w = w\n",
    "\n",
    "        # bounded sigmoid function, this is the probability estimation\n",
    "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
    "\n",
    "    def update(self, x, p, y):\n",
    "        ''' Update model using x, p, y\n",
    "\n",
    "            INPUT:\n",
    "                x: feature, a list of indices\n",
    "                p: click probability prediction of our model\n",
    "                y: answer\n",
    "\n",
    "            MODIFIES:\n",
    "                self.n: increase by squared gradient\n",
    "                self.z: weights\n",
    "        '''\n",
    "\n",
    "        # parameter\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "\n",
    "        # gradient under logloss\n",
    "        g = p - y\n",
    "\n",
    "        # update z and n\n",
    "        for i in self._indices(x):\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(p, y):\n",
    "    ''' FUNCTION: Bounded logloss\n",
    "\n",
    "        INPUT:\n",
    "            p: our prediction\n",
    "            y: real answer\n",
    "\n",
    "        OUTPUT:\n",
    "            logarithmic loss of p given y\n",
    "    '''\n",
    "\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(path, D):\n",
    "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
    "                   and for simplicity, we one-hot-encode everything\n",
    "\n",
    "        INPUT:\n",
    "            path: path to training or testing file\n",
    "            D: the max index that we can hash to\n",
    "\n",
    "        YIELDS:\n",
    "            ID: id of the instance, mainly useless\n",
    "            x: a list of hashed and one-hot-encoded 'indices'\n",
    "               we only need the index since all values are either 0 or 1\n",
    "            y: y = 1 if we have a click, else we have y = 0\n",
    "    '''\n",
    "\n",
    "    for t, row in enumerate(DictReader(open(path))):\n",
    "        # process id\n",
    "        disp_id = int(row['display_id'])\n",
    "        ad_id = int(row['ad_id'])\n",
    "\n",
    "        # process clicks\n",
    "        y = 0.\n",
    "        if 'clicked' in row:\n",
    "            if row['clicked'] == '1':\n",
    "                y = 1.\n",
    "            del row['clicked']\n",
    "\n",
    "        x = []\n",
    "        for key in row:\n",
    "            x.append(abs(hash(key + '_' + row[key])) % D)\n",
    "\n",
    "        row = prcont_dict.get(ad_id, [])\t\t\n",
    "        # build x\n",
    "        ad_doc_id = -1\n",
    "        for ind, val in enumerate(row):\n",
    "            if ind==0:\n",
    "                ad_doc_id = int(val)\n",
    "            x.append(abs(hash(prcont_header[ind] + '_' + val)) % D)\n",
    "\n",
    "        row = event_dict.get(disp_id, [])\n",
    "        ## build x\n",
    "        disp_doc_id = -1\n",
    "        for ind, val in enumerate(row):\n",
    "            if ind==0:\n",
    "                uuid_val = val\n",
    "            if ind==1:\n",
    "                disp_doc_id = int(val)\n",
    "            x.append(abs(hash(event_header[ind] + '_' + val)) % D)\n",
    "            \n",
    "        yield t, disp_id, ad_id, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content..\n",
      "559583\n",
      "Events..\n",
      "23120126\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# initialize ourselves a learner\n",
    "learner = ftrl_proximal(alpha, beta, L1, L2, D, interaction)\n",
    "\n",
    "print(\"Content..\")\n",
    "with open(data_path + \"promoted_content.csv\") as infile:\n",
    "    prcont = csv.reader(infile)\n",
    "    prcont_header = next(prcont)[1:]\n",
    "    prcont_dict = {}\n",
    "    for ind,row in enumerate(prcont):\n",
    "        prcont_dict[int(row[0])] = row[1:]\n",
    "    print(len(prcont_dict))\n",
    "del prcont\n",
    "\n",
    "print(\"Events..\")\n",
    "with open(data_path + \"events.csv\") as infile:\n",
    "    events = csv.reader(infile)\n",
    "    next(events)\n",
    "    event_header = ['uuid', 'document_id', 'platform', 'geo_location', 'loc_country', 'loc_state', 'loc_dma']\n",
    "    event_dict = {}\n",
    "    for ind,row in enumerate(events):\n",
    "        tlist = row[1:3] + row[4:6]\n",
    "        loc = row[5].split('>')\n",
    "        if len(loc) == 3:\n",
    "            tlist.extend(loc[:])\n",
    "        elif len(loc) == 2:\n",
    "            tlist.extend( loc[:]+[''])\n",
    "        elif len(loc) == 1:\n",
    "            tlist.extend( loc[:]+['',''])\n",
    "        else:\n",
    "            tlist.append(['','',''])\t\n",
    "        event_dict[int(row[0])] = tlist[:] \n",
    "    print(len(event_dict))\n",
    "del events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-08 10:27:14.082354\tencountered: 2500000\tcurrent logloss: 0.469359\n",
      "2016-12-08 10:28:41.983290\tencountered: 5000000\tcurrent logloss: 0.455599\n",
      "2016-12-08 10:30:13.084636\tencountered: 7500000\tcurrent logloss: 0.453785\n",
      "2016-12-08 10:31:43.148627\tencountered: 10000000\tcurrent logloss: 0.451383\n",
      "2016-12-08 10:33:14.174139\tencountered: 12500000\tcurrent logloss: 0.448328\n",
      "2016-12-08 10:34:42.534015\tencountered: 15000000\tcurrent logloss: 0.447515\n",
      "2016-12-08 10:36:13.073220\tencountered: 17500000\tcurrent logloss: 0.447139\n",
      "2016-12-08 10:37:41.476856\tencountered: 20000000\tcurrent logloss: 0.444607\n",
      "2016-12-08 10:39:09.579793\tencountered: 22500000\tcurrent logloss: 0.444527\n",
      "2016-12-08 10:40:37.066381\tencountered: 25000000\tcurrent logloss: 0.443691\n",
      "2016-12-08 10:42:05.781939\tencountered: 27500000\tcurrent logloss: 0.443197\n",
      "2016-12-08 10:43:34.882696\tencountered: 30000000\tcurrent logloss: 0.444163\n",
      "2016-12-08 10:45:03.962984\tencountered: 32500000\tcurrent logloss: 0.443890\n",
      "2016-12-08 10:46:32.086398\tencountered: 35000000\tcurrent logloss: 0.444042\n",
      "2016-12-08 10:48:02.913731\tencountered: 37500000\tcurrent logloss: 0.443915\n",
      "2016-12-08 10:49:28.814587\tencountered: 40000000\tcurrent logloss: 0.443280\n",
      "2016-12-08 10:50:57.159034\tencountered: 42500000\tcurrent logloss: 0.442931\n",
      "2016-12-08 10:52:25.242742\tencountered: 45000000\tcurrent logloss: 0.442151\n",
      "2016-12-08 10:53:54.072233\tencountered: 47500000\tcurrent logloss: 0.442207\n",
      "2016-12-08 10:55:21.944430\tencountered: 50000000\tcurrent logloss: 0.442366\n",
      "2016-12-08 10:56:49.248312\tencountered: 52500000\tcurrent logloss: 0.441599\n",
      "2016-12-08 10:58:17.635752\tencountered: 55000000\tcurrent logloss: 0.441779\n",
      "2016-12-08 10:59:47.763488\tencountered: 57500000\tcurrent logloss: 0.441615\n",
      "2016-12-08 11:01:13.660982\tencountered: 60000000\tcurrent logloss: 0.441105\n",
      "2016-12-08 11:02:42.336823\tencountered: 62500000\tcurrent logloss: 0.440952\n",
      "2016-12-08 11:04:11.983145\tencountered: 65000000\tcurrent logloss: 0.440814\n",
      "2016-12-08 11:05:38.879163\tencountered: 67500000\tcurrent logloss: 0.440628\n",
      "2016-12-08 11:07:05.453955\tencountered: 70000000\tcurrent logloss: 0.440926\n",
      "2016-12-08 11:08:37.888064\tencountered: 72500000\tcurrent logloss: 0.440629\n",
      "2016-12-08 11:10:06.360331\tencountered: 75000000\tcurrent logloss: 0.440295\n",
      "2016-12-08 11:11:37.251072\tencountered: 77500000\tcurrent logloss: 0.440250\n",
      "2016-12-08 11:13:07.582795\tencountered: 80000000\tcurrent logloss: 0.439779\n",
      "2016-12-08 11:14:35.364976\tencountered: 82500000\tcurrent logloss: 0.439709\n",
      "2016-12-08 11:16:02.070552\tencountered: 85000000\tcurrent logloss: 0.439882\n",
      "Epoch 0 finished, holdout logloss: 0.439718, elapsed time: 0:53:44.571304\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for e in range(epoch):\n",
    "    loss = 0.\n",
    "    count = 0\n",
    "    date = 0\n",
    "\n",
    "    for t, disp_id, ad_id, x, y in data(train, D):  # data is a generator\n",
    "        #    t: just a instance counter\n",
    "        # date: you know what this is\n",
    "        #   ID: id provided in original data\n",
    "        #    x: features\n",
    "        #    y: label (click)\n",
    "\n",
    "        # step 1, get prediction from learner\n",
    "        p = learner.predict(x)\n",
    "\n",
    "        if t % holdout == 0:\n",
    "            # step 2-1, calculate validation loss\n",
    "            #           we do not train with the validation data so that our\n",
    "            #           validation loss is an accurate estimation\n",
    "            #\n",
    "            # holdafter: train instances from day 1 to day N\n",
    "            #            validate with instances from day N + 1 and after\n",
    "            #\n",
    "            # holdout: validate with every N instance, train with others\n",
    "            loss += logloss(p, y)\n",
    "            count += 1\n",
    "        else:\n",
    "            # step 2-2, update learner with label (click) information\n",
    "            learner.update(x, p, y)\n",
    "\n",
    "        if t % 2500000 == 0 and t > 1:\n",
    "            print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "                datetime.now(), t, loss/count))\n",
    "            \n",
    "    print('Epoch %d finished, holdout logloss: %f, elapsed time: %s' % (\n",
    "        e, loss/count, str(datetime.now() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processed : ', 0, datetime.datetime(2016, 12, 8, 11, 17, 25, 950266))\n",
      "('Processed : ', 1000000, datetime.datetime(2016, 12, 8, 11, 17, 52, 356982))\n",
      "('Processed : ', 2000000, datetime.datetime(2016, 12, 8, 11, 18, 18, 800496))\n",
      "('Processed : ', 3000000, datetime.datetime(2016, 12, 8, 11, 18, 45, 289026))\n"
     ]
    }
   ],
   "source": [
    "with open(submission, 'w') as outfile:\n",
    "    outfile.write('display_id,ad_id,clicked\\n')\n",
    "    for t, disp_id, ad_id, x, y in data(test, D):\n",
    "        p = learner.predict(x)\n",
    "        outfile.write('%s,%s,%s\\n' % (disp_id, ad_id, str(p)))\n",
    "        if t%1000000 == 0:\n",
    "            print(\"Processed : \", t, datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
