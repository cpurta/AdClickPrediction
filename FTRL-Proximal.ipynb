{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ftrl_proximal(object):\n",
    "    ''' Our main algorithm: Follow the regularized leader - proximal\n",
    "\n",
    "        In short,\n",
    "        this is an adaptive-learning-rate sparse logistic-regression with\n",
    "        efficient L1-L2-regularization\n",
    "\n",
    "        Reference:\n",
    "        http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, alpha, beta, L1, L2, D, interaction=False):\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "\n",
    "        # feature related parameters\n",
    "        self.D = D\n",
    "        self.interaction = interaction\n",
    "\n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        self.n = [0.] * D\n",
    "        self.z = [0.] * D\n",
    "        self.w = [0.] * D  # use this for execution speed up\n",
    "        # self.w = {}  # use this for memory usage reduction\n",
    "\n",
    "    def _indices(self, x):\n",
    "        ''' A helper generator that yields the indices in x\n",
    "\n",
    "            The purpose of this generator is to make the following\n",
    "            code a bit cleaner when doing feature interaction.\n",
    "        '''\n",
    "\n",
    "        for i in x:\n",
    "            yield i\n",
    "\n",
    "        if self.interaction:\n",
    "            D = self.D\n",
    "            L = len(x)\n",
    "            for i in xrange(1, L):  # skip bias term, so we start at 1\n",
    "                for j in xrange(i+1, L):\n",
    "                    yield (i * j) % D\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' Get probability estimation on x\n",
    "\n",
    "            INPUT:\n",
    "                x: features\n",
    "\n",
    "            OUTPUT:\n",
    "                probability of p(y = 1 | x; w)\n",
    "        '''\n",
    "\n",
    "        # parameters\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w  # use this for execution speed up\n",
    "        # w = {}  # use this for memory usage reduction\n",
    "\n",
    "        # wTx is the inner product of w and x\n",
    "        wTx = 0.\n",
    "        for i in self._indices(x):\n",
    "            sign = -1. if z[i] < 0 else 1.  # get sign of z[i]\n",
    "\n",
    "            # build w on the fly using z and n, hence the name - lazy weights -\n",
    "            if sign * z[i] <= L1:\n",
    "                # w[i] vanishes due to L1 regularization\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                # apply prediction time L1, L2 regularization to z and get w\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "\n",
    "            wTx += w[i]\n",
    "\n",
    "        self.w = w\n",
    "\n",
    "        # bounded sigmoid function, this is the probability estimation\n",
    "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
    "\n",
    "    def update(self, x, p, y):\n",
    "        ''' Update model using x, p, y\n",
    "\n",
    "            INPUT:\n",
    "                x: feature, a list of indices\n",
    "                p: click probability prediction of our model\n",
    "                y: answer\n",
    "\n",
    "            MODIFIES:\n",
    "                self.n: increase by squared gradient\n",
    "                self.z: weights\n",
    "        '''\n",
    "\n",
    "        # parameter\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w  # no need to change this, it won't gain anything\n",
    "\n",
    "        # gradient under logloss\n",
    "        g = p - y\n",
    "\n",
    "        # update z and n\n",
    "        for i in self._indices(x):\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(p, y):\n",
    "    ''' FUNCTION: Bounded logloss\n",
    "\n",
    "        INPUT:\n",
    "            p: our prediction\n",
    "            y: real answer\n",
    "\n",
    "        OUTPUT:\n",
    "            logarithmic loss of p given y\n",
    "    '''\n",
    "\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class logistic_regression(object):\n",
    "    ''' Classical logistic regression\n",
    "    \n",
    "        This class (algorithm) is not used in this code, it is putted here\n",
    "        for a quick reference in hope to make the following (more complex)\n",
    "        algorithm more understandable.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, alpha, D, interaction=False):\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # model\n",
    "        self.w = [0.] * D\n",
    "\n",
    "    def predict(self, x):\n",
    "        # parameters\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # model\n",
    "        w = self.w\n",
    "\n",
    "        # wTx is the inner product of w and x\n",
    "        wTx = sum(w[i] for i in x)\n",
    "\n",
    "        # bounded sigmoid function, this is the probability of being clicked\n",
    "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
    "\n",
    "    def update(self, x, p, y):\n",
    "        # parameter\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # model\n",
    "        w = self.w\n",
    "\n",
    "        # gradient under logloss\n",
    "        g = p - y\n",
    "\n",
    "        # update w\n",
    "        for i in x:\n",
    "            w[i] += g * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(path, D):\n",
    "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
    "                   and for simplicity, we one-hot-encode everything\n",
    "\n",
    "        INPUT:\n",
    "            path: path to training or testing file\n",
    "            D: the max index that we can hash to\n",
    "\n",
    "        YIELDS:\n",
    "            ID: id of the instance, mainly useless\n",
    "            x: \n",
    "            y: y = 1 if we have a click, else we have y = 0\n",
    "    '''\n",
    "    \n",
    "    for t, row in enumerate(DictReader(open(path))):\n",
    "        # process id\n",
    "        ID = row['ad_id']\n",
    "        del row['ad_id']\n",
    "        \n",
    "        DID = row['display_id']\n",
    "\n",
    "        # process clicks\n",
    "        y = 0.\n",
    "        if 'clicked' in row:\n",
    "            if row['clicked'] == '1':\n",
    "                y = 1.\n",
    "            del row['clicked']\n",
    "\n",
    "        # build x\n",
    "        x = [0]  # 0 is the index of the bias term\n",
    "        for key in sorted(row):  # sort is for preserving feature ordering\n",
    "            value = row[key]\n",
    "\n",
    "            # one-hot encode everything with hash trick\n",
    "            index = abs(hash(key + '_' + value)) % D\n",
    "            x.append(index)\n",
    "\n",
    "        yield t, ID, DID, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A, paths\n",
    "train = './input/clicks_meta_train.csv' # path to training file\n",
    "test = './input/clicks_meta_test.csv'   # path to testing file\n",
    "submission = './output/submission_ftrl.csv'    # path of to be outputted submission file\n",
    "\n",
    "# B, model\n",
    "alpha = .1  # learning rate\n",
    "beta = 1.   # smoothing parameter for adaptive learning rate\n",
    "L1 = 0.     # L1 regularization, larger value means more regularized\n",
    "L2 = 0.     # L2 regularization, larger value means more regularized\n",
    "\n",
    "# C, feature/hash trick\n",
    "D = 2 ** 20              # number of weights to use\n",
    "do_interactions = True  # whether to enable poly2 feature interactions\n",
    "\n",
    "# D, training/validation\n",
    "epoch = 1      # learn training data for N passes\n",
    "holdout = 100  # use every N training instance for holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTRL-Proximal Results:\n"
     ]
    }
   ],
   "source": [
    "# initialize ourselves a learner\n",
    "learner = ftrl_proximal(alpha, beta, L1, L2, D, interaction=do_interactions)\n",
    "\n",
    "print('FTRL-Proximal Results:')\n",
    "\n",
    "# start training\n",
    "for e in xrange(epoch):\n",
    "    loss = 0.\n",
    "    count = 0\n",
    "\n",
    "    start = datetime.now()\n",
    "    for t, ID, DID, x, y in data(train, D):  # data is a generator\n",
    "        #  t: just a instance counter\n",
    "        # ID: id provided in original data\n",
    "        #  x: features\n",
    "        #  y: label (click)\n",
    "\n",
    "        # step 1, get prediction from learner\n",
    "        p = learner.predict(x)\n",
    "\n",
    "        if t % holdout == 0:\n",
    "            # step 2-1, calculate holdout validation loss\n",
    "            #           we do not train with the holdout data so that our\n",
    "            #           validation loss is an accurate estimation of\n",
    "            #           the out-of-sample error\n",
    "            loss += logloss(p, y)\n",
    "            count += 1\n",
    "        else:\n",
    "            # step 2-2, update learner with label (click) information\n",
    "            learner.update(x, p, y)\n",
    "\n",
    "        if t % 2500000 == 0 and t > 1:\n",
    "            print(' %s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "                datetime.now(), t, loss/count))\n",
    "\n",
    "    print('Epoch %d finished, holdout logloss: %f, elapsed time: %s' % (\n",
    "        e, loss/count, str(datetime.now() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test set ad_ids...\n",
      "2016-12-06 10:38:36.849095\tPredicted 0 records\n",
      "2016-12-06 10:38:57.893033\tPredicted 1000000 records\n",
      "2016-12-06 10:39:18.460915\tPredicted 2000000 records\n",
      "2016-12-06 10:39:39.248627\tPredicted 3000000 records\n",
      "2016-12-06 10:39:59.483774\tPredicted 4000000 records\n",
      "2016-12-06 10:40:19.862966\tPredicted 5000000 records\n",
      "2016-12-06 10:40:40.382918\tPredicted 6000000 records\n",
      "2016-12-06 10:41:00.773774\tPredicted 7000000 records\n",
      "2016-12-06 10:41:21.053760\tPredicted 8000000 records\n",
      "2016-12-06 10:41:41.390628\tPredicted 9000000 records\n",
      "2016-12-06 10:42:01.679597\tPredicted 10000000 records\n",
      "2016-12-06 10:42:22.260057\tPredicted 11000000 records\n",
      "2016-12-06 10:42:43.284403\tPredicted 12000000 records\n",
      "2016-12-06 10:43:03.613602\tPredicted 13000000 records\n",
      "2016-12-06 10:43:23.808892\tPredicted 14000000 records\n",
      "2016-12-06 10:43:44.659209\tPredicted 15000000 records\n",
      "2016-12-06 10:44:05.819746\tPredicted 16000000 records\n",
      "2016-12-06 10:44:26.871222\tPredicted 17000000 records\n",
      "2016-12-06 10:44:47.745350\tPredicted 18000000 records\n",
      "2016-12-06 10:45:09.253738\tPredicted 19000000 records\n",
      "2016-12-06 10:45:30.737274\tPredicted 20000000 records\n",
      "2016-12-06 10:45:52.270257\tPredicted 21000000 records\n",
      "2016-12-06 10:46:13.406822\tPredicted 22000000 records\n",
      "2016-12-06 10:46:33.593924\tPredicted 23000000 records\n",
      "2016-12-06 10:46:53.686526\tPredicted 24000000 records\n",
      "2016-12-06 10:47:13.489323\tPredicted 25000000 records\n",
      "2016-12-06 10:47:33.503544\tPredicted 26000000 records\n",
      "2016-12-06 10:47:53.592521\tPredicted 27000000 records\n",
      "2016-12-06 10:48:13.297424\tPredicted 28000000 records\n",
      "2016-12-06 10:48:33.623196\tPredicted 29000000 records\n",
      "2016-12-06 10:48:53.725039\tPredicted 30000000 records\n",
      "2016-12-06 10:49:13.836928\tPredicted 31000000 records\n",
      "2016-12-06 10:49:34.274255\tPredicted 32000000 records\n"
     ]
    }
   ],
   "source": [
    "print('Predicting test set ad_ids...')\n",
    "\n",
    "with open(submission, 'w') as outfile:\n",
    "    outfile.write('display_id,ad_id,clicked\\n')\n",
    "    for t, ID, DID, x, y in data(test, D):\n",
    "        p = learner.predict(x)\n",
    "        outfile.write('%s,%s,%s\\n' % (DID,ID,str(p)))\n",
    "        if t % 1000000 == 0:\n",
    "            print('%s\\tPredicted %d records' % (datetime.now(), t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
